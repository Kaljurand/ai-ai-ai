# Evaluate Estonian ASR

## Introduction

This specification describes a tool (outlined in this introduction) and instructions for its use (detailed in the following sections). The tool is a command-line interface (CLI) application designed to run efficiently on a Unix command line, but it should also provide a visual web-based UI that lists all generated files.

The purpose of the tool is to evaluate various ASR (Automatic Speech Recognition) models for Estonian. The process involves generating multiple input texts using different text generation models, converting these texts to audio using TTS (Text-to-Speech) models (prompted in different ways), and then transcribing the audio with multiple ASR models. Each transcription ("hyp") is then compared to its original input ("ref") using an evaluation model. The simplest evaluation is the calculation of word-error-rate (WER) between "ref" and "hyp" (using the "builtin/wer" model). More advanced semantic evaluation can be performed by an LLM, which is instructed to use the WER tool and provide a semantic summary of the differences.

This specification includes:

- Text generation models and prompts
- Text-to-speech models and prompts
- Audio transcription models and prompts

Each section lists one or more models (in the Models section), exactly one meta prompt, and zero or more prompts (as subsections under Prompts, if present). All possible combinations should be generated. The "meta prompt" must be used as the instruction or system prompt; but if the provider does not support this, it should be prepended to each prompt.

The system should:
- Use the Text section to generate reference texts with all models and prompts.
- Use the Text-to-speech section to convert all generated texts into audio, using all available models and prompts.
- Use the Audio transcription section to transcribe all generated audio, using all available models and prompts.
- Evaluate how well each transcription ("hyp") matches the original text ("ref"), using all available evaluation models and prompts.

All generated files should be stored and made accessible via a browser-based interface, using a human-readable naming convention. For example, `text_2_weather:tts_1_fast:asr_3:eval_2` denotes the evaluation result produced by evaluation model 2, which compared "text_5_weather" (the reference text generated by the 5th text generation model using the "weather" prompt) against the transcription obtained by first generating audio with TTS model 1 (using the "fast" prompt), and then transcribing it with the 3rd ASR model (without any prompt).

In this context, the input text of a given pipline run is referred to as $ref, and the output of the audio transcription is $hyp. These placeholders should be replaced with the actual values before calling the evaluation model.

Implement the tool using Python libraries such as LangChain and rich. The tool should first load this specification (from the value of the `--spec` option), parsing it with a standard Markdown library. It should then execute the specification by querying all models in parallel, while displaying progress on the command line, for example:

```
text_1_weather:tts_1_fast...
text_1_weather:tts_1_slow...
text_2_weather:tts_1_fast:asr_3...
text_5_weather:tts_1_fast:asr_3:eval_2 => 3 sec, WER=0.05
...
```

Each API call that is in progress is indicated by "...". When a pipeline finishes, it ends with "=>" followed by the total duration, the word-error rate, total number of consumed tokens (if available), and a brief evaluation report (if available).

The tool should support a mock mode (`--mock`), where API calls are not actually made and dummy outputs are generated instead, allowing the pipeline to proceed for testing and demonstration purposes.

The tool should also support re-calculating a given branch of the specification (e.g. `text_1_weather:tts_1_slow` means that `tts_1_slow` and all the following steps should be calculated), assuming previously calculated files in the output directory (i.e. `text_1_weather` in this case).

## Text

### Models

1. openai/gpt-4.1-nano
2. openrouter/grok-3-mini

### Meta prompt

Generate an Estonian text to be used as input for TTS. Guidelines:

- Do not act as or use a chat model.
- Only generate the text itself, without any explanations or extra scaffolding.
- You may use Markdown formatting and Elevenlabs-style meta-tags such as "[whispers]".

### Prompts

#### Teen

Genereeri dialoog kahe teismelise vahel. Esimene on 80ndade nolk, kelle jutt on t채is venekeelseid roppusi. Teine on 2000ndate idufirma asutaja, kes miksib jutu sisse ingliskeelseid s천nu ("pivot", "pitch"), mille eestikeelset t천lget ta ei tea.

#### Weather

Generate a short weather report, with lots of abbreviations.

## Text-to-speech

### Models

1. openai/gpt-4o-mini-tts

### Meta prompt

The text is in Estonian, but may contain foreign words, which should be pronounced with an Estonian accent. The text may also include "voice tags" such as "[whispers]". Expand all abbreviations before speaking; for example, "25 km/h-ni" should be read as "kahek체mne viie kilomeetrini tunnis" to improve clarity.

### Prompts

#### Fast

Read it fast and use some traffic noise in the background.

#### Finnish

Read it with a Finnish accent.

## Audio transcription

### Models

1. openai/whisper-1
2. openai/gpt-4o-transcribe

### Meta prompt

Transcribe this Estonian audio. Add punctuation marks. Use standard orthography (write numbers as digits and use common abbreviations).

## Evaluation

### Models

1. builtin/wer
2. openrouter/grok-3-mini

### Meta prompt

Perform a semantic comparison of the following texts: "ref" (the correct reference text) and "hyp" (the automatic ASR transcription of the audio of "ref"). Note that "hyp" may contain various ASR errors.

```
<ref>$ref</ref>

<hyp>$hyp</hyp>
```

Also use the WER tool to calculate the word-error-rate. Report the result as JSON with the following fields:

- wer
- text of evaluation
